{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Book - Examples.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PozQ3m4dF7sJ",
        "colab_type": "text"
      },
      "source": [
        "# 70 years of machine learning in geoscience \n",
        "Accompanies https://arxiv.org/abs/2006.13311\n",
        "## Example code\n",
        "\n",
        "These are code snippets that were included to show the developments of machine learning tools in the 2010s. The strides Python and the machine learning community has made with a consolidated ML API in scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5sbVQuGZCG",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "In the following, we use the utility functions of scikit-learn to prepare a classification data set and split the data into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB72xAyoTJ8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=5000, n_features=5,\n",
        "                           n_informative=3, n_redundant=0,\n",
        "                           random_state=0, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKtfVLUvfdds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vZLzdy5Gm09",
        "colab_type": "text"
      },
      "source": [
        "### Support-Vector Machines\n",
        "Train a SVM, predict, score, and evaluate feature importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEMtDZiWTQjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(random_state=0, probability=True)\n",
        "svm.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw72xUIeTRh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(svm.predict([[0, 0, 0, 0, 0], [-1, -1, -1, -1, -1], [1, 1, 1, 1, 1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhSJMwidft4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(svm.score(X_train, y_train))\n",
        "print(svm.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgieb5wTndQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "importances = permutation_importance(svm, X_train, y_train, n_repeats=10, random_state=0)\n",
        "print(importances.importances_mean)\n",
        "print(importances.importances_mean.argsort())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbKobNM_GrIQ",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier\n",
        "Train a Random Forest, predict, score, and evaluate feature importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv2b9CBwTSSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(max_depth=7, random_state=0)\n",
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVStJvHT3S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(rf.predict([[0, 0, 0, 0, 0], [-1, -1, -1, -1, -1], [1, 1, 1, 1, 1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L2yTBTGT35X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(rf.feature_importances_)\n",
        "print(rf.feature_importances_.argsort())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjftHPpbUBb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(rf.score(X_train, y_train))\n",
        "print(rf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wiwYsAxGztp",
        "colab_type": "text"
      },
      "source": [
        "### Gaussian Processes\n",
        "Train two Gaussian Processes with different kernels, predict, score, and evaluate feature importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOxSvkZTTBJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import DotProduct as DP\n",
        "gp = GaussianProcessClassifier(random_state=0, copy_X_train=False)\n",
        "gp0 = GaussianProcessClassifier(kernel=DP(.25), random_state=0, copy_X_train=False)\n",
        "gp.fit(X_train[:1000], y_train[:1000])\n",
        "gp0.fit(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvA8dBjTQiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(gp.predict([[0, 0, 0, 0, 0], [-1, -1, -1, -1, -1], [1, 1, 1, 1, 1]]))\n",
        "print(gp0.predict([[0, 0, 0, 0, 0], [-1, -1, -1, -1, -1], [1, 1, 1, 1, 1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmbTXqQQTRDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(gp.score(X_train, y_train))\n",
        "print(gp.score(X_test, y_test))\n",
        "print(gp0.score(X_train, y_train))\n",
        "print(gp0.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNL5GdRJHEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "importances = permutation_importance(gp0, X_train, y_train, n_repeats=10, random_state=0)\n",
        "print(importances.importances_mean)\n",
        "print(importances.importances_mean.argsort())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZydeW4CDTk8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "importances = permutation_importance(gp, X_train, y_train, n_repeats=10, random_state=0)\n",
        "print(importances.importances_mean)\n",
        "print(importances.importances_mean.argsort())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWg_LZMjG2jc",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning\n",
        "Train a deep neural network, predict, and score accuracy. Then train the same model 10 times with different initializations to evaluate the convergence of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJRBf_LeE_zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnG5dtFWzuNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(900)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdL-NT1SE2Wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "tf.keras.layers.Dense(32, activation='relu'),\n",
        "tf.keras.layers.Dropout(.3),\n",
        "tf.keras.layers.Dense(16, activation='relu'),\n",
        "tf.keras.layers.Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_yHn1tpF-wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxUpJTwJGOzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, \n",
        "          y_train, \n",
        "          validation_split=.1,\n",
        "          epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDkVJDrFGTWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5fUPvCHNTd",
        "colab_type": "text"
      },
      "source": [
        "Here we train ten models on ten initializations to evaluate the convergence of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtG7T3J0R2Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "seeds = 10\n",
        "\n",
        "train_loss = np.zeros((seeds,100))\n",
        "val_loss = np.zeros((seeds,100))\n",
        "train_acc = np.zeros((seeds,100))\n",
        "val_acc = np.zeros((seeds,100))\n",
        "\n",
        "for i in range(seeds):\n",
        "    tf.keras.backend.clear_session()\n",
        "    del model\n",
        "    tf.random.set_seed(i*100)\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(.3),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    history = model.fit(X_train, \n",
        "              y_train, \n",
        "              validation_split=.1,\n",
        "              epochs=100,\n",
        "              verbose=0)\n",
        "    \n",
        "    \n",
        "    train_loss[i,:] = history.history['loss']\n",
        "    val_loss[i,:] = history.history['val_loss']\n",
        "    train_acc[i,:] = history.history['accuracy']\n",
        "    val_acc[i,:] = history.history['val_accuracy']\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpnqLn1jTxeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_u = np.mean(train_loss, axis=0)\n",
        "train_loss_s = np.std(train_loss, axis=0)\n",
        "val_loss_u = np.mean(val_loss, axis=0)\n",
        "val_loss_s = np.std(val_loss, axis=0)\n",
        "train_acc_u = np.mean(train_acc, axis=0)\n",
        "train_acc_s = np.std(train_acc, axis=0)\n",
        "val_acc_u = np.mean(val_acc, axis=0)\n",
        "val_acc_s = np.std(val_acc, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykYzho4OHFwd",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzgjovbYHxl_",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRKXXbncHKeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cycler import cycler\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize='x-small')\n",
        "plt.rc('ytick', labelsize='x-small')\n",
        "plt.rc('axes', prop_cycle=(cycler(color=['#67001f', '#053061'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBpd6lX7VHy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "fig, axs = plt.subplots(figsize=(8,3), ncols=2)\n",
        "x = list(range(100))\n",
        "\n",
        "axs[1].fill_between(x, train_acc_u-train_acc_s*2, train_acc_u+train_acc_s*2,\n",
        "    alpha=.2, facecolor='C0',)\n",
        "axs[1].fill_between(x, val_acc_u-val_acc_s*2, val_acc_u+val_acc_s*2,\n",
        "    alpha=.2, facecolor='C1',)\n",
        "axs[1].plot(x,train_acc_u, color='C0', linewidth=1)\n",
        "axs[1].plot(x,val_acc_u, color='C1', linewidth=1)\n",
        "axs[1].set_title('Neural Network Accuracy')\n",
        "axs[1].set_ylabel('Metric: Accuracy')\n",
        "axs[1].set_xlabel('Training Epoch')\n",
        "\n",
        "# summarize history for loss\n",
        "axs[0].fill_between(x, train_loss_u-train_loss_s*2, train_loss_u+train_loss_s*2,\n",
        "    alpha=.2, facecolor='C0')\n",
        "axs[0].fill_between(x, val_loss_u-val_loss_s*2, val_loss_u+val_loss_s*2,\n",
        "    alpha=.2, facecolor='C1',)\n",
        "axs[0].plot(x,train_loss_u, color='C0', linewidth=1)\n",
        "axs[0].plot(x,val_loss_u, color='C1', linewidth=1)\n",
        "axs[0].set_title('Neural Network Loss')\n",
        "axs[0].set_ylabel('Loss: Cross-Entropy')\n",
        "axs[0].set_xlabel('Training Epoch')\n",
        "axs[1].legend(['Training Data', 'Validation Data', '2σ contains 95%', '2σ contains 95%'], loc='best')\n",
        "fig.subplots_adjust(left=0.01)\n",
        "fig.savefig('nn-loss.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HaCi7uBH390",
        "colab_type": "text"
      },
      "source": [
        "### Classifier Decision Surfaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTiPzN3hU-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers = {\"SVM\": svm, \"Gaussian Process\": gp, \"Random Forest\": rf, \"DNN\": model}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT3JHouzu8Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mesh to calculate all predictions on\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "x_min, x_mean, x_max = X[:, 0].min() - .5, np.mean(X[:, 0]), X[:, 0].max() + .5\n",
        "y_min, y_mean, y_max = X[:, 1].min() - .5, np.mean(X[:, 1]), X[:, 1].max() + .5\n",
        "z_min, z_mean, z_max = X[:, 2].min() - .5, np.mean(X[:, 2]), X[:, 2].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJLchRSJJg_V",
        "colab_type": "text"
      },
      "source": [
        "The following function plots the decision surfaces with their respective inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Di5X5IFYBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sklearn_slice(classifiers, offset, h=0.02, out_file=None):\n",
        "    \"\"\" Plot input data and 2D decision surfaces of 3D data\n",
        "\n",
        "    classifiers (dict): Different classifiers to portray. Key will be used as \n",
        "                        title and value should be the fitted model.\n",
        "    offset (float):     Position of 2D slice withing the 3D volume\n",
        "    h (float):          Size of 2D slice\n",
        "    out_file (str):     Filename of image to save to.\n",
        "    \"\"\"\n",
        "    i = 1\n",
        "\n",
        "    #z_slice = (z_min, z_max) #all\n",
        "    z_slice = (offset-h, offset+h) #specific slice\n",
        "\n",
        "    z_bool = (X[:,2] > z_slice[0]) * (X[:,2] < z_slice[1])\n",
        "    z_train_bool = (X_train[:,2] > z_slice[0]) * (X_train[:,2] < z_slice[1])\n",
        "    z_test_bool = (X_test[:,2] > z_slice[0]) * (X_test[:,2] < z_slice[1])\n",
        "\n",
        "    cm = plt.cm.RdBu\n",
        "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "    figure = plt.figure(figsize=(4 * (len(classifiers) + 1), 4))\n",
        "    ax = plt.subplot(1, len(classifiers) + 1, i)\n",
        "    ax.set_title(\"Input data\")\n",
        "\n",
        "    # Plot all data points\n",
        "    all_plot = ax.scatter(X[np.logical_not(z_bool), 0], X[np.logical_not(z_bool), 1], \n",
        "                          facecolor='#C0C0C0', label=\"All Data\")\n",
        "    # Plot the training points\n",
        "    train_plot = ax.scatter(X_train[z_train_bool, 0], X_train[z_train_bool, 1], c=y_train[z_train_bool], \n",
        "                            cmap=cm_bright, edgecolors='k', label=\"Training Data\")\n",
        "    # Plot the testing points\n",
        "    test_plot = ax.scatter(X_test[z_test_bool, 0], X_test[z_test_bool, 1], c=y_test[z_test_bool], \n",
        "                           cmap=cm_bright, marker='x', label=\"Test Data\")\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    grey_patch = mpatches.Patch(color='#C0C0C0', label='All Data (3D)')\n",
        "    red_patch = mpatches.Patch(color='#FF0000', label='Class 0')\n",
        "    blue_patch = mpatches.Patch(color='#0000FF', label='Class 1')\n",
        "    ax.legend(handles=[grey_patch, red_patch, blue_patch, train_plot, test_plot], loc='best')\n",
        "    i += 1\n",
        "    # iterate over classifiers\n",
        "    for name, clf in classifiers.items():\n",
        "        ax = plt.subplot(1, len(classifiers) + 1, i)\n",
        "        try:\n",
        "            score = clf.score(X_test, y_test)\n",
        "        except:\n",
        "            score = clf.evaluate(X_test, y_test, verbose=0)[1]\n",
        "        # Plot the decision boundary. For that, we will assign a color to each\n",
        "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "        # if hasattr(clf, \"decision_function\"):\n",
        "        #     Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel(), np.zeros_like(yy.ravel())+offset, np.zeros_like(yy.ravel()), np.zeros_like(yy.ravel())])\n",
        "        # elif hasattr(clf, \"predict_classes\"):\n",
        "        zrs = np.zeros_like(yy.ravel())\n",
        "        if hasattr(clf, \"predict_classes\"):\n",
        "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel(), zrs+offset, zrs, zrs], verbose=0)[:, 1]\n",
        "        else:\n",
        "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), zrs+offset, zrs, zrs])[:, 1]\n",
        "        \n",
        "        # Put the result into a color plot\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "        \n",
        "        # Plot the training points\n",
        "        ax.scatter(X_train[z_train_bool, 0], X_train[z_train_bool, 1], c=y_train[z_train_bool], \n",
        "                   cmap=cm_bright, edgecolors='k')\n",
        "        # Plot the testing points\n",
        "        ax.scatter(X_test[z_test_bool, 0], X_test[z_test_bool, 1], c=y_test[z_test_bool], \n",
        "                   cmap=cm_bright, marker='x')\n",
        "        \n",
        "        ax.set_xlim(xx.min(), xx.max())\n",
        "        ax.set_ylim(yy.min(), yy.max())\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        ax.set_title(name)\n",
        "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
        "                size=15, horizontalalignment='right')\n",
        "        i += 1\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    if out_file is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(out_file, bbox_inches='tight')\n",
        "    plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpKkqnNvZBxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sklearn_slice(classifiers, z_mean, out_file=\"decision-boundaries.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJte8YM6n3Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sklearn_slice({\"Linear Kernel\": gp0, \"RBF Kernel\": gp}, z_mean, out_file=\"gaussian-processes.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlvAxlYW3fq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iterate over classifiers\n",
        "for name, clf in classifiers.items():\n",
        "    sklearn_slice({name: clf}, z_mean, out_file=name+\".png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ff04bfIQ4Z",
        "colab_type": "text"
      },
      "source": [
        "### Video of 3D decision volume\n",
        "Iterate over entire volume, create an image for each, then collect all images and create a movie from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEi1jj71xhAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imageio\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "drivepath = \"losses/\"\n",
        "\n",
        "for x, off in enumerate(tqdm(np.arange(z_min+h, z_max-h, h))):\n",
        "    sklearn_slice(classifiers, off, out_file=f\"{drivepath}loss3D-{x:02d}.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clCrEqzGylfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = []\n",
        "for i in range(439):\n",
        "    images.append(imageio.imread(f\"{drivepath}loss3D-{i:02d}.png\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue0_x0EA7asj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageio.mimsave('decision_boundary2.mp4', images[::3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R12wgVjGQ08J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageio.mimsave('decision_boundary2.gif', images)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}